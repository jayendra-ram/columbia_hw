{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Task 2: Regularizations\n",
    "\n",
    "In this task, you are going to experiment with two popular regularization methods. \n",
    "\n",
    "**Batch normalization:** Batch normalization makes it possible to train a deep network. When a network becomes deeper, the distribution of hidden neurons' values will also shift greatly, and that is one reason that makes it difficult to train a deep neural network. Machine learning taught us that normalization is a good preprocessing method to deal with such a problem. Therefore, batch normalization deploys a similar idea in the neural network by re-normalizing the hidden values of each layer before transfering values to the next layer.\n",
    "\n",
    "**Dropout:** In the last assignment, you trained a shallow network and everything looked fine. However, when the network becomes larger and deeper, it will also become harder to train. The first potential problem is over-fitting, that is, the network overreacts to noise or random errors of the training data while failing to detect the underlying distribution pattern. It is more likely to occur when the model becomes complex and contains more trainable parameters. Dropout is a well-known method that can eliminate such effects. The core idea behind it is quite simple: rather than updating all trainable parameters each time, it randomly selects a subset of parameters to update and keeps other parameters unaltered.\n",
    "\n",
    "* References\n",
    "    * https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "    * https://arxiv.org/pdf/1502.03167.pdf\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (60000, 784)\n",
      "Training labels shape:  (60000,)\n",
      "Validation data shape:  (10000, 784)\n",
      "Validation labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, val = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_val_raw, y_val = val\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_val = X_val_raw.reshape((X_val_raw.shape[0], X_val_raw.shape[1]**2))\n",
    "\n",
    "#Index from the 10000th image of the dataset\n",
    "# X_val = X_train[10000:10500,:]\n",
    "# y_val = y_train[10000:10500]\n",
    "# X_train = X_train[10500:12500,:]\n",
    "# y_train = y_train[10500:12500]\n",
    "\n",
    "mean_image = np.mean(X_train, axis=0).astype(np.float32)\n",
    "X_train = X_train.astype(np.float32) - mean_image\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "# We've vectorized the data for you. That is, we flatten the 32×32×3 images into 1×3072 Numpy arrays.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement batch normalization foward function\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span> Edit functions **bn_forward** in **./utils/reg_funcs.py**. <br> If the code is running correctly, **mean of a2_bn for train will be very close to 0 and variance of a2_bn will be close to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of a2:  [-3.71855056  3.21810258 -2.44388986]\n",
      "var of a2:  [45.32564755 70.17112882 32.35120115]\n",
      "(train) mean of a2_bn:  [-1.93178806e-16 -1.84297022e-16 -1.12132525e-16]\n",
      "(train) var of a2_bn:  [0.99999978 0.99999986 0.99999969]\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# Checking/verification code. Don't change it.     #\n",
    "####################################################\n",
    "\n",
    "from utils.reg_funcs import bn_forward\n",
    "from utils.reg_funcs import bn_backward\n",
    "\n",
    "np.random.seed(1338)\n",
    "N, D, H1, H2 = 300, 20, 8, 3\n",
    "x_in = np.random.randn(N, D)\n",
    "w1 = np.random.randn(D,H1)\n",
    "w2 = np.random.randn(H1,H2)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "\n",
    "# Before batch normalization\n",
    "print(\"mean of a2: \", np.mean(a2, axis=0))\n",
    "print(\"var of a2: \", np.var(a2, axis=0))\n",
    "\n",
    "# Test \"train mode\" of forward function\n",
    "# After batch normalization, the mean should be close to zero and var should be close to one. \n",
    "bn_config = {\"epsilon\":1e-5, \"decay\":0.9}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "a2_bn, _ = bn_forward(a2, gamma, beta, bn_config, \"train\")\n",
    "print(\"(train) mean of a2_bn: \", np.mean(a2_bn, axis=0))\n",
    "print(\"(train) var of a2_bn: \", np.var(a2_bn, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Test \"moving average\" and \"test mode\" of the bn_forward function. <br> We expect that the test mean & variance of a2 to be fairly close to the real values and the test mean & variance of a2_bn to be close to 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real mean of data:  [-4.64127543  3.58614096 -2.81356668]\n",
      "real var of data:  [51.58165787 68.82075512 34.09234473]\n",
      "moving mean of data:  [-4.53266176  3.52274298 -2.86954586]\n",
      "moving var of data:  [51.13187379 70.01558829 33.30406561]\n",
      "********************************************************************************\n",
      "(test) mean of a2:  [-4.62543722  3.51559486 -2.96856329]\n",
      "(test) var of a2:  [58.81281991 59.36184418 29.94072842]\n",
      "(test) mean of a2_bn:  [-5.02931030e-16 -3.84877315e-16 -1.97619698e-16]\n",
      "(test) var of a2_bn:  [0.99999983 0.99999983 0.99999967]\n",
      "(300, 3)\n",
      "(300, 3)\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# Checking code. Don't change it.     #\n",
    "#######################################\n",
    "\n",
    "# Test \"moving average\" and \"test mode\" of forward function\n",
    "# Then you are going to run the forward function under \"training mode\" for several times, \n",
    "# and the moving mean and moving var will be close to the real mean and var of the input data.\n",
    "# Next, run the forward function under \"test\" mode and you will see that the mean and var of its \n",
    "# output will be also close to gamma, beta that you have set before.\n",
    "\n",
    "bn_config = {\"epsilon\":1e-5, \"decay\":0.9}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "\n",
    "# collect_data: for calculating real mean and var of a2 later.\n",
    "collect_data = a2\n",
    "for _ in range(100):\n",
    "    x_in = np.random.randn(N, D)\n",
    "    a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "    collect_data = np.concatenate((collect_data, a2), axis=0)\n",
    "    bn_forward(a2, gamma, beta, bn_config, \"train\")\n",
    "\n",
    "# compare moving_mean and moving_var with real mean and var.\n",
    "# You should see that they are close to each other.\n",
    "print(\"real mean of data: \", np.mean(collect_data, axis=0))\n",
    "print(\"real var of data: \", np.var(collect_data, axis=0))\n",
    "print(\"moving mean of data: \", bn_config[\"moving_mean\"])\n",
    "print(\"moving var of data: \", bn_config[\"moving_var\"])\n",
    "\n",
    "# \"test mode\" of forward function\n",
    "# After bn_forward, the mean and var of output should be kind of close to gamma and beta.\n",
    "x_in = np.random.randn(N, D)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "print(\"*\"*80)\n",
    "print(\"(test) mean of a2: \", np.mean(a2, axis=0))\n",
    "print(\"(test) var of a2: \", np.var(a2, axis=0))\n",
    "a2_bn, _ = bn_forward(a2, gamma, beta, bn_config, \"train\")\n",
    "print(\"(test) mean of a2_bn: \", np.mean(a2_bn, axis=0))\n",
    "print(\"(test) var of a2_bn: \", np.var(a2_bn, axis=0))\n",
    "\n",
    "print(a2.shape)\n",
    "print(a2_bn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Use TensorFlow 2 functions to verify the correctness of the backward function. You should use functions from TensorFlow 2 to get the gradients. \n",
    "<br>**Hint: use tf.GradientTape()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float64'>\n",
      "<dtype: 'float64'>\n",
      "<dtype: 'float64'>\n",
      "Is a2_bn correct? False\n",
      "Is da2 correct? True\n",
      "Is dgamma correct? True\n",
      "Is dbeta correct? True\n"
     ]
    }
   ],
   "source": [
    "# After verifying the forward function and save the bn_config.\n",
    "x_in = np.random.randn(N, D)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "da2_bn = np.ones_like(a2)\n",
    "# Test backward function with tensorflow\n",
    "# You will use bn_config = {\"eps\":1e-5, \"decay\":0.9, \"moving_mean\":moving_mean, \"moving_var\":moving_var}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "a2_bn, cache = bn_forward(a2, gamma, beta, bn_config, \"test\")\n",
    "da2, dgamma, dbeta = bn_backward(da2_bn, cache)\n",
    "\n",
    "\n",
    "###################################################\n",
    "# TODO: verify the backward code. You should use  #\n",
    "# the parameters in bn_config and other variables #\n",
    "# above.                                          #\n",
    "###################################################\n",
    "a2_tf= tf.Variable(a2)\n",
    "gamma_tf = tf.Variable(gamma)\n",
    "beta_tf = tf.Variable(beta)\n",
    "\n",
    "x, gamma, beta, eps, mean, var = cache\n",
    "#eps, mean, var = bn_config[\"epsilon\"], bn_config[\"moving_mean\"], bn_config[\"moving_var\"]\n",
    "print(a2_tf.dtype)\n",
    "print(gamma_tf.dtype)\n",
    "print(beta_tf.dtype)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(a2_tf)\n",
    "    a2_bn_check = tf.nn.batch_normalization(\n",
    "        a2_tf ,mean, var, beta_tf, gamma_tf, eps)\n",
    "    da2_check, dgamma_check,dbeta_check = tape.gradient(a2_bn_check,(a2_tf,gamma_tf,beta_tf))\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "# ENDTODO #\n",
    "###################################################\n",
    "    \n",
    "# Make comparison\n",
    "print(\"Is a2_bn correct? {}\".format(np.allclose(a2_bn, a2_bn_check)))\n",
    "print(\"Is da2 correct? {}\".format(np.allclose(da2, da2_check)))\n",
    "print(\"Is dgamma correct? {}\".format(np.allclose(dgamma, dgamma_check)))\n",
    "print(\"Is dbeta correct? {}\".format(np.allclose(dbeta, dbeta_check)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization Experiments with MLP\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span>\n",
    "\n",
    "1. Add batch normalization into MLP. Please understand **loss**, **predict** functions of the class **MLP** in **./utils/neuralnets/mlp.py**\n",
    "\n",
    "2. First create a shallow MLP like two-layer network like [100,10]. Train it without and with batch normalization. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "3. Then, create a 5-layer MLP network like [100,50,50,50,10] and train the network with and without batch normalization. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "4. Make a comparison and describe what you have found in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment on shallow MLP** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.neuralnets.mlp import MLP \n",
    "from utils.optimizers import AdamOptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 600\n",
      "epoch 1: valid acc = 0.8358, new learning rate = 0.00095\n",
      "epoch 2: valid acc = 0.8475, new learning rate = 0.0009025\n",
      "epoch 3: valid acc = 0.8511, new learning rate = 0.000857375\n",
      "epoch 4: valid acc = 0.8585, new learning rate = 0.0008145062499999999\n",
      "epoch 5: valid acc = 0.8659, new learning rate = 0.0007737809374999998\n"
     ]
    }
   ],
   "source": [
    "# Build a two-layer network without batch normalization.\n",
    "# Here is a demo.\n",
    "use_bn = False\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_shallow_no_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=1e-3, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 600\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (100,100) into shape (100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nc/dy9m49ms4gz3kmgw95x5srtw0000gn/T/ipykernel_58296/1695332131.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamOptim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# hist_no_dropout contains loss, train acc and valid acc history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m hist_shallow_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n\u001b[0m\u001b[1;32m      8\u001b[0m                            \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                            verbose=False, record_interval = 4)\n",
      "\u001b[0;32m~/projects/e4040-2021fall-assign2-jcr2211/utils/optimizers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, X_train, y_train, X_valid, y_valid, num_epoch, batch_size, learning_rate, learning_decay, verbose, record_interval)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# update model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/e4040-2021fall-assign2-jcr2211/utils/neuralnets/mlp.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# Affine backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"affine_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weight_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bias_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/e4040-2021fall-assign2-jcr2211/utils/layer_funcs.py\u001b[0m in \u001b[0;36maffine_backward\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mx_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_flatten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (100,100) into shape (100)"
     ]
    }
   ],
   "source": [
    "# Build a two-layer network with batch normalization. Remember to \"use_bn\".\n",
    "use_bn = True\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_shallow_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=1e-3, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bn and no_bn results together in three plots and make a comparison. \n",
    "title_name = [\"loss\", \"train acc\", \"val acc\"]\n",
    "_, axarr = plt.subplots(1,3, figsize=(15,5))\n",
    "for i in range(3):\n",
    "    axarr[i].plot(hist_shallow_no_bn[i], label=\"no_bn\")\n",
    "    axarr[i].plot(hist_shallow_bn[i], label=\"bn\")\n",
    "    axarr[i].legend(), axarr[i].set_title(title_name[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment on deep MLP** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a deep 5-layer network without batch normalization. Remember to \"use_bn\".\n",
    "use_bn = False\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 50, 50, 50], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_no_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=2e-4, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a deep 5-layer network with batch normalization. Remember to \"use_bn\".\n",
    "use_bn = True\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 50, 50, 50], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=2e-4, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bn and no_bn results together in three plots and make a comparison. \n",
    "title_name = [\"loss\", \"train acc\", \"val acc\"]\n",
    "_, axarr = plt.subplots(1,3, figsize=(15,5))\n",
    "for i in range(3):\n",
    "    axarr[i].plot(hist_deep_no_bn[i], label=\"no_bn\")\n",
    "    axarr[i].plot(hist_deep_bn[i], label=\"bn\")\n",
    "    axarr[i].legend(), axarr[i].set_title(title_name[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Describe what you find in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: **[fill in here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement dropout_forward function\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span> Edit function **dropout_forward** in **./utils/reg_funcs.py**. If the code is running correctly, you will see that the outputs of the verification code should be close to each other. \n",
    "<br><br> If the function is correct, then **the output mean should be close to the input mean. Input mean and output test mean should be identical.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = np.random.randn(500, 500) + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_of_input = 6.999505818459608\n",
      "mean_of_out = 9.979144697000013\n",
      "mean_of_out_test = 9.999294026370869\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Checking/verification code. Don't change it. #\n",
    "################################################\n",
    "from utils.reg_funcs import dropout_forward\n",
    "from utils.reg_funcs import dropout_backward\n",
    "\n",
    "p = 0.7\n",
    "dropout_config = {\"enabled\": True, \"keep_prob\": p}\n",
    "# feedforward\n",
    "out, cache = dropout_forward(x=x_in, dropout_config=dropout_config, mode=\"train\")\n",
    "out_test, _ = dropout_forward(x=x_in, dropout_config=dropout_config, mode=\"test\") \n",
    "# backward\n",
    "dout = np.ones_like(x_in)\n",
    "dx = dropout_backward(dout, cache)\n",
    "################################################\n",
    "# Checking/verification code. Don't change it. #\n",
    "################################################\n",
    "# Check forward correctness\n",
    "print(\"mean_of_input = {}\".format(p*np.mean(x_in)))\n",
    "print(\"mean_of_out = {}\".format(np.mean(out)))\n",
    "print(\"mean_of_out_test = {}\".format(np.mean(out_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Experiments with MLP\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span>\n",
    "\n",
    "1. Add dropout into the MLP. Review **loss**, **predict** functions of class **MLP** in **./utils/neuralnets/mlp.py** and understand how the dropout is added into the MLP.\n",
    "\n",
    "2. Customize your own MLP network.Then, train networks with different **keep_prob**, like 0.1, 0.3, 0.5, 0.7, 0.9, 1. If **keep_prob**==1, then the network is the MLP without dropout.\n",
    "\n",
    "3. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "Note that checking/validation code is included below with preselected dropout parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 600\n",
      "epoch 1: valid acc = 0.8502, new learning rate = 0.00095\n",
      "epoch 2: valid acc = 0.8535, new learning rate = 0.0009025\n"
     ]
    }
   ],
   "source": [
    "# Here is an example on how to collect loss and accuracy info\n",
    "dropout_config = {\"enabled\":True, \"keep_prob\": 1}\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_no_dropout = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=2, batch_size=100, learning_rate=1e-3, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 600\n",
      "epoch 1: valid acc = 0.8375, new learning rate = 0.00095\n",
      "epoch 2: valid acc = 0.8471, new learning rate = 0.0009025\n"
     ]
    }
   ],
   "source": [
    "dropout_config[\"keep_prob\"] = 0.9\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_9 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=2, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 600\n",
      "epoch 1: valid acc = 0.8577, new learning rate = 0.00095\n",
      "epoch 2: valid acc = 0.8627, new learning rate = 0.0009025\n"
     ]
    }
   ],
   "source": [
    "dropout_config[\"keep_prob\"] = 0.7\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_7 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=2, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 600\n",
      "epoch 1: valid acc = 0.8451, new learning rate = 0.00095\n",
      "epoch 2: valid acc = 0.8534, new learning rate = 0.0009025\n"
     ]
    }
   ],
   "source": [
    "dropout_config[\"keep_prob\"] = 0.5\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_5 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=2, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 600\n",
      "epoch 1: valid acc = 0.8469, new learning rate = 0.00095\n",
      "epoch 2: valid acc = 0.8555, new learning rate = 0.0009025\n"
     ]
    }
   ],
   "source": [
    "dropout_config[\"keep_prob\"] = 0.3\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_3 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=2, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 600\n",
      "epoch 1: valid acc = 0.8425, new learning rate = 0.00095\n",
      "epoch 2: valid acc = 0.8507, new learning rate = 0.0009025\n"
     ]
    }
   ],
   "source": [
    "# small retention rate\n",
    "dropout_config[\"keep_prob\"] = 0.1\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_1 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=2, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_no_dropout, train_acc_no_dropout, val_acc_no_dropout = hist_no_dropout\n",
    "loss_dropout_1, train_acc_dropout_1, val_acc_dropout_1 = hist_dropout_1\n",
    "loss_dropout_3, train_acc_dropout_3, val_acc_dropout_3 = hist_dropout_3\n",
    "loss_dropout_5, train_acc_dropout_5, val_acc_dropout_5 = hist_dropout_5\n",
    "loss_dropout_7, train_acc_dropout_7, val_acc_dropout_7 = hist_dropout_7\n",
    "loss_dropout_9, train_acc_dropout_9, val_acc_dropout_9 = hist_dropout_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_no_dropout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nc/dy9m49ms4gz3kmgw95x5srtw0000gn/T/ipykernel_55861/2280583061.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_no_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"no dropout\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dropout_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"keep_prob=0.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dropout_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"keep_prob=0.3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dropout_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"keep_prob=0.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dropout_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"keep_prob=0.7\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_no_dropout' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(loss_no_dropout, label=\"no dropout\")\n",
    "plt.plot(loss_dropout_1, label=\"keep_prob=0.1\")\n",
    "plt.plot(loss_dropout_3, label=\"keep_prob=0.3\")\n",
    "plt.plot(loss_dropout_5, label=\"keep_prob=0.5\")\n",
    "plt.plot(loss_dropout_7, label=\"keep_prob=0.7\")\n",
    "plt.plot(loss_dropout_9, label=\"keep_prob=0.9\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc_no_dropout, label=\"no dropout\")\n",
    "plt.plot(train_acc_dropout_1, label=\"keep_prob=0.1\")\n",
    "plt.plot(train_acc_dropout_3, label=\"keep_prob=0.3\")\n",
    "plt.plot(train_acc_dropout_5, label=\"keep_prob=0.5\")\n",
    "plt.plot(train_acc_dropout_7, label=\"keep_prob=0.7\")\n",
    "plt.plot(train_acc_dropout_9, label=\"keep_prob=0.9\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_acc_no_dropout, label=\"no dropout\")\n",
    "plt.plot(val_acc_dropout_1, label=\"keep_prob=0.1\")\n",
    "plt.plot(val_acc_dropout_3, label=\"keep_prob=0.3\")\n",
    "plt.plot(val_acc_dropout_5, label=\"keep_prob=0.5\")\n",
    "plt.plot(val_acc_dropout_7, label=\"keep_prob=0.7\")\n",
    "plt.plot(val_acc_dropout_9, label=\"keep_prob=0.9\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Describe what you find in this dropout experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: **[fill in here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Dropout + Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep network with both dropout and batch normalization.\n",
    "dropout_config = {\"enabled\":True, \"keep_prob\": 0.7}\n",
    "use_bn = True\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 50], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=200, learning_rate=1e-3, learning_decay=1.0, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "* Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks https://arxiv.org/abs/1602.07868\n",
    "* Highway netowrk https://arxiv.org/pdf/1505.00387.pdf"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
